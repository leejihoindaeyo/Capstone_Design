{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv(r'C:\\Users\\user\\Desktop\\DataSet\\total.csv')\n",
    "data = data.dropna()  # 데이터의 결측값을 판단\n",
    "\n",
    "# Amplitude 열의 수를 확인하여 반복\n",
    "amplitude_columns = [f'Amplitude_{i + 1}' for i in range(89)]\n",
    "\n",
    "# 평균, 최대값, 최소값, 표준편차를 저장할 리스트\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# 전체 데이터에 대한 통계량 계산\n",
    "for index, row in data.iterrows():\n",
    "    # 현재 Amplitude 값들을 가져오기\n",
    "    amplitudes = [row[col] for col in amplitude_columns]\n",
    "\n",
    "    # 통계량 계산\n",
    "    mean_amplitude = sum(amplitudes) / len(amplitudes)\n",
    "    std_amplitude = pd.Series(amplitudes).std()\n",
    "    max_amplitude = max(amplitudes)\n",
    "    min_amplitude = min(amplitudes)\n",
    "\n",
    "    # 현재 샘플의 TYPE\n",
    "    material_type = row['TYPE']\n",
    "\n",
    "    # 특징 벡터 생성\n",
    "    feature_vector = {\n",
    "        'mean': mean_amplitude,\n",
    "        'std': std_amplitude,\n",
    "        'max': max_amplitude,\n",
    "        'min': min_amplitude,\n",
    "        'label': 0 if material_type == 'HARD' else 1  # HARD는 0, SOFT는 1로 라벨링\n",
    "    }\n",
    "\n",
    "    features.append(feature_vector)\n",
    "\n",
    "# DataFrame으로 변환\n",
    "features_df = pd.DataFrame(features)\n",
    "\n",
    "# 전처리된 특징을 CSV 파일로 저장\n",
    "features_df.to_csv(r'C:\\Users\\user\\Desktop\\DataSet\\processed_features.csv', index=False)\n",
    "\n",
    "# 데이터 타입 확인\n",
    "X = features_df.drop(columns=['label']).values\n",
    "y = features_df['label'].astype(int).values\n",
    "\n",
    "# CNN 입력 데이터 형태로 변환\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)  # (샘플 수, 특성 수, 1)\n",
    "\n",
    "# K-Fold Cross Validation 설정\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "all_train_loss = []\n",
    "all_val_loss = []\n",
    "all_train_accuracy = []\n",
    "all_val_accuracy = []\n",
    "all_cm = np.zeros((2, 2))\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # 모델 구축\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))  # 입력층\n",
    "    model.add(MaxPooling1D(pool_size=2))  # 풀링층\n",
    "    model.add(Flatten())  # 평탄화\n",
    "    model.add(Dense(32, activation='relu'))  # 은닉층\n",
    "    model.add(Dense(1, activation='sigmoid'))  # 출력층 (이진 분류)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # 모델 훈련\n",
    "    history = model.fit(X_train, y_train, epochs=40, batch_size=16, validation_data=(X_val, y_val))\n",
    "\n",
    "    # 모델 성능 평가\n",
    "    loss, accuracy = model.evaluate(X_val, y_val)\n",
    "    print(f'Fold {fold + 1} - Validation Loss: {loss}, Validation Accuracy: {accuracy}')\n",
    "\n",
    "    # 혼동 행렬\n",
    "    y_val_pred = (model.predict(X_val) >= 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_val, y_val_pred)\n",
    "    all_cm += cm  # 혼동 행렬 합산\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['HARD', 'SOFT'], yticklabels=['HARD', 'SOFT'])\n",
    "    plt.title(f'Confusion Matrix for Fold {fold + 1}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    # 분류 리포트\n",
    "    print(classification_report(y_val, y_val_pred, target_names=['HARD', 'SOFT']))\n",
    "\n",
    "    # 손실과 정확도 저장\n",
    "    all_train_loss.append(history.history['loss'])\n",
    "    all_val_loss.append(history.history['val_loss'])\n",
    "    all_train_accuracy.append(history.history['accuracy'])\n",
    "    all_val_accuracy.append(history.history['val_accuracy'])\n",
    "\n",
    "# 평균 손실과 정확도 계산\n",
    "mean_train_loss = np.mean(all_train_loss, axis=0)\n",
    "mean_val_loss = np.mean(all_val_loss, axis=0)\n",
    "mean_train_accuracy = np.mean(all_train_accuracy, axis=0)\n",
    "mean_val_accuracy = np.mean(all_val_accuracy, axis=0)\n",
    "\n",
    "# 평균 러닝 커브 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mean_train_loss, label='Mean Train Loss')\n",
    "plt.plot(mean_val_loss, label='Mean Validation Loss')\n",
    "plt.title('Mean Loss per Epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(mean_train_accuracy, label='Mean Train Accuracy')\n",
    "plt.plot(mean_val_accuracy, label='Mean Validation Accuracy')\n",
    "plt.title('Mean Accuracy per Epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 평균 혼동 행렬 시각화\n",
    "average_cm = all_cm / 5  # 5개의 fold에 대한 평균 혼동 행렬\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(average_cm, annot=True, fmt='.2f', cmap='Blues', xticklabels=['HARD', 'SOFT'], yticklabels=['HARD', 'SOFT'])\n",
    "plt.title('Average Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
